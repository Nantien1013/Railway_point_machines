{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeaa17bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Environment summary ===\n",
      "Time: 2026-02-09T23:42:39\n",
      "OS: Ubuntu 22.04.5 LTS\n",
      "CPU: Intel(R) Xeon(R) Gold 5215 CPU @ 2.50GHz\n",
      "RAM: 125.4 GB\n",
      "\n",
      "GPU(s) (nvidia-smi):\n",
      "  - GPU 0: Tesla T4 (15.0 GB VRAM), driver 535.247.01, CUDA(driver) 12.2\n",
      "  - GPU 1: Tesla T4 (15.0 GB VRAM), driver 535.247.01, CUDA(driver) 12.2\n",
      "  - GPU 2: Tesla T4 (15.0 GB VRAM), driver 535.247.01, CUDA(driver) 12.2\n",
      "  - GPU 3: Tesla T4 (15.0 GB VRAM), driver 535.247.01, CUDA(driver) 12.2\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# Hardware / software environment (CPU/GPU/RAM/torch/cuda/cudnn)\n",
    "# =======================\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def _run_cmd(cmd):\n",
    "    \"\"\"Run command and return stdout (str). Never raises; returns '' on failure.\"\"\"\n",
    "    try:\n",
    "        out = subprocess.check_output(cmd, stderr=subprocess.STDOUT, text=True)\n",
    "        return out.strip()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def get_cpu_model_linux():\n",
    "    # Try /proc/cpuinfo first (most reliable on Linux)\n",
    "    txt = _run_cmd([\"bash\", \"-lc\", \"cat /proc/cpuinfo | grep -m1 'model name'\"])\n",
    "    if txt:\n",
    "        # model name : Intel(R) Xeon...\n",
    "        return txt.split(\":\", 1)[-1].strip()\n",
    "    # Fallback: lscpu\n",
    "    txt = _run_cmd([\"bash\", \"-lc\", \"lscpu | grep -m1 'Model name'\"])\n",
    "    if txt:\n",
    "        return txt.split(\":\", 1)[-1].strip()\n",
    "    return \"N/A\"\n",
    "\n",
    "def get_total_ram_gb_linux():\n",
    "    txt = _run_cmd([\"bash\", \"-lc\", \"grep -m1 MemTotal /proc/meminfo\"])\n",
    "    if not txt:\n",
    "        return \"N/A\"\n",
    "    # MemTotal:       32708940 kB\n",
    "    m = re.search(r\"MemTotal:\\s+(\\d+)\\s+kB\", txt)\n",
    "    if not m:\n",
    "        return \"N/A\"\n",
    "    kb = int(m.group(1))\n",
    "    gb = kb / (1024**2)  # kB -> GB (GiB-ish)\n",
    "    return f\"{gb:.1f} GB\"\n",
    "\n",
    "def get_gpu_info_nvidia_smi():\n",
    "    \"\"\"\n",
    "    Returns list of dicts: [{\"name\":..., \"vram_gb\":..., \"driver\":..., \"cuda_reported\":...}, ...]\n",
    "    If not available, returns [].\n",
    "    \"\"\"\n",
    "    # Check nvidia-smi availability\n",
    "    smi = _run_cmd([\"bash\", \"-lc\", \"command -v nvidia-smi\"])\n",
    "    if not smi:\n",
    "        return []\n",
    "\n",
    "    # Query name and memory\n",
    "    q = \"nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader,nounits\"\n",
    "    out = _run_cmd([\"bash\", \"-lc\", q])\n",
    "    if not out:\n",
    "        return []\n",
    "\n",
    "    gpus = []\n",
    "    for line in out.splitlines():\n",
    "        parts = [p.strip() for p in line.split(\",\")]\n",
    "        if len(parts) >= 3:\n",
    "            name, mem_mb, driver = parts[0], parts[1], parts[2]\n",
    "            try:\n",
    "                vram_gb = float(mem_mb) / 1024.0\n",
    "                vram_str = f\"{vram_gb:.1f} GB\"\n",
    "            except Exception:\n",
    "                vram_str = \"N/A\"\n",
    "            gpus.append({\"name\": name, \"vram\": vram_str, \"driver\": driver})\n",
    "\n",
    "    # Also try to read \"CUDA Version\" reported by nvidia-smi header\n",
    "    header = _run_cmd([\"bash\", \"-lc\", \"nvidia-smi | head -n 3\"])\n",
    "    cuda_ver = \"N/A\"\n",
    "    m = re.search(r\"CUDA Version:\\s*([\\d.]+)\", header)\n",
    "    if m:\n",
    "        cuda_ver = m.group(1)\n",
    "\n",
    "    for g in gpus:\n",
    "        g[\"cuda_reported_by_driver\"] = cuda_ver\n",
    "    return gpus\n",
    "\n",
    "def get_torch_cuda_cudnn_info():\n",
    "    info = {\n",
    "        \"python\": platform.python_version(),\n",
    "        \"pytorch\": \"N/A\",\n",
    "        \"cuda_available\": \"N/A\",\n",
    "        \"torch_cuda\": \"N/A\",\n",
    "        \"cudnn\": \"N/A\",\n",
    "        \"cudnn_enabled\": \"N/A\",\n",
    "        \"gpu_count_torch\": \"N/A\",\n",
    "    }\n",
    "    try:\n",
    "        import torch\n",
    "        info[\"pytorch\"] = torch.__version__\n",
    "        info[\"cuda_available\"] = bool(torch.cuda.is_available())\n",
    "        info[\"torch_cuda\"] = torch.version.cuda if torch.version.cuda is not None else \"N/A\"\n",
    "        info[\"cudnn\"] = torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else \"N/A\"\n",
    "        info[\"cudnn_enabled\"] = bool(torch.backends.cudnn.enabled)\n",
    "        info[\"gpu_count_torch\"] = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "    except Exception:\n",
    "        pass\n",
    "    return info\n",
    "\n",
    "def get_os_info():\n",
    "    # More detailed OS info for Linux\n",
    "    os_pretty = _run_cmd([\"bash\", \"-lc\", \"cat /etc/os-release 2>/dev/null | grep -m1 PRETTY_NAME\"])\n",
    "    if os_pretty:\n",
    "        return os_pretty.split(\"=\", 1)[-1].strip().strip('\"')\n",
    "    return f\"{platform.system()} {platform.release()}\"\n",
    "\n",
    "def collect_environment():\n",
    "    env = {}\n",
    "    env[\"timestamp\"] = datetime.now().isoformat(timespec=\"seconds\")\n",
    "    env[\"os\"] = get_os_info()\n",
    "    env[\"cpu\"] = get_cpu_model_linux()\n",
    "    env[\"ram\"] = get_total_ram_gb_linux()\n",
    "\n",
    "    gpus = get_gpu_info_nvidia_smi()\n",
    "    env[\"gpus\"] = gpus  # list\n",
    "    env[\"torch\"] = get_torch_cuda_cudnn_info()\n",
    "    return env\n",
    "\n",
    "env = collect_environment()\n",
    "\n",
    "# --- Print in a table-like way ---\n",
    "print(\"=== Environment summary ===\")\n",
    "print(\"Time:\", env[\"timestamp\"])\n",
    "print(\"OS:\", env[\"os\"])\n",
    "print(\"CPU:\", env[\"cpu\"])\n",
    "print(\"RAM:\", env[\"ram\"])\n",
    "\n",
    "if env[\"gpus\"]:\n",
    "    print(\"\\nGPU(s) (nvidia-smi):\")\n",
    "    for i, g in enumerate(env[\"gpus\"]):\n",
    "        print(f\"  - GPU {i}: {g['name']} ({g['vram']} VRAM), driver {g['driver']}, CUDA(driver) {g['cuda_reported_by_driver']}\")\n",
    "else:\n",
    "    print(\"\\nGPU(s): N/A (no nvidia-smi or no NVIDIA GPU detected)\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "switch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
